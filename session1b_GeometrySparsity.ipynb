{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e36eb7-e334-4c0f-86fa-7be1a8729306",
   "metadata": {},
   "source": [
    "# Session 1b - Inmas Workshop Machine Learning Workshop, January 13-14, 2024\n",
    "\n",
    "Instructor: Christian Kuemmerle - kuemmerle@uncc.edu "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc2ec3-7f08-401c-bb42-2ab9986282f6",
   "metadata": {},
   "source": [
    "## High-Dimensional Geometry, Sparse Regression, Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1260a",
   "metadata": {},
   "source": [
    "First, we load packages and functions that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e31243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7af2b",
   "metadata": {},
   "source": [
    "## Geometry of High-Dimensional Spaces\n",
    "\n",
    "First, we explore the phenomenon of concentration of measure in high dimensions through simple examples as a warm-up.\n",
    "\n",
    "We compare the behavior of multivariate normal distribution in high dimensions with low-dimensional ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be1cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random number generator with a fixed seed for reproducibility\n",
    "rng = np.random.default_rng(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f190d2f",
   "metadata": {},
   "source": [
    "**Randomly generate 1000 points from an $d$-dimensional standard Gaussian distribution and plot the histogram of the distances between those 1000 points and the origin. Do this for $d=1$, $d=2$, $d=100$ and $d=50000$.** <br>\n",
    "\n",
    "Hint: You can use `numpy.random.randn` to sample standard Gaussians and `numpy.linalg.norm` to compute vector norms. You can look into the `numpy` documentation for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d6bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add your code here ###\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e126aa",
   "metadata": {},
   "source": [
    "What do the histograms tell us?\n",
    "\n",
    "The high-dimensional normal distribution is close to a spherical-type of distribution, whereas the $2$-dimensional one is rather spread out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691f5cf",
   "metadata": {},
   "source": [
    "**Randomly generate two sets of 500 points from a $d$-dimensional standard gaussian distribution and plot the histogram of the pairwise distances between the two sets with $d=1$, $d=2$, $d=100$ and $d=50000$.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e90b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random number generator with a fixed seed for reproducibility\n",
    "rng = np.random.default_rng(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd09984",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [1, 2,100,5000]\n",
    "n = 500\n",
    "npoints = n**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ec0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add your code here ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aee85a",
   "metadata": {},
   "source": [
    "What does the histogram tell us?\n",
    "\n",
    "The pairwise distances of i.i.d. samples of a high-dimensional Gaussian concentrate very much for large $d$. In particular, this means that there are very few points that are \"close\" to each other in high dimensions! They seem to have almost all roughly the same distance to each other, which is remarkable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97094b54",
   "metadata": {},
   "source": [
    "## The geometry of the high-dimensional sphere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c250cf9",
   "metadata": {},
   "source": [
    "**Randomly generate 1000 points uniformly from a 100-dimensional unit ball and plot the histogram of the distances between those 1000 points and the origin.** <br> <br>\n",
    "**Hint:** There are several ways how to solve this problem.\n",
    "   - One of the uses high-dimensional Gaussian distribution, see also: https://stackoverflow.com/questions/5408276/sampling-uniformly-distributed-random-points-inside-a-spherical-volume/\n",
    "   - Another possibility includes _rejection sampling_: To generate one point, one can uniformly draw from the unit box with radius $1$, and reject samples that are far enough away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gammainc\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create a random number generator with a fixed seed for reproducibility\n",
    "rng = np.random.default_rng(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for generating npoints points uniformly from a n-dimensional unit ball\n",
    "# Hint: use high-dimensional gaussian distribution and incomplete gamma function to generate the points, see https://stackoverflow.com/questions/5408276/sampling-uniformly-distributed-random-points-inside-a-spherical-volume/ for details\n",
    "def sample_ball(npoints,n):\n",
    "    p = \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sample_ball(1000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 20\n",
    "fig3, ax3 = plt.subplots(1, sharey=True, tight_layout=True)\n",
    "# We can set the number of bins with the *bins* keyword argument.\n",
    "ax3.hist(np.linalg.norm(p, axis=1), bins=n_bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c1431",
   "metadata": {},
   "source": [
    "**What does the histogram tell us?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8cb29-d1ce-48b2-862f-4b8786226f9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51823782",
   "metadata": {},
   "source": [
    "# Sparse Regression\n",
    "\n",
    "We return to the [Boston housing dataset](https://www.openml.org/d/531) and apply ideas from sparse regularization in order to obtain more interpretable models (feature selection).\n",
    "\n",
    "### Lasso regression with cross-validated regularization  paramter (hyperparameter optimization via grid search)\n",
    "\n",
    "We will apply the described procedure for [Lasso regression](https://web.stanford.edu/~hastie/StatLearnSparsity/) (\"Least absolute shrinkage and selection operator\"), which promotes _sparsity_ in the coefficients of the linear model. As the learned model tends to have a sparse coefficient vector, it leads to a better _interpretability_ of the model as we can identify which features are used in the model, and which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428afe40",
   "metadata": {},
   "source": [
    "First, we load again the data, and put it into a `pandas.DataFrame` data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f24a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Load data set\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO']\n",
    "df_boston = pd.DataFrame(data[:,0:11],columns = column_names)\n",
    "df_boston.assign(MEDV=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe80b3",
   "metadata": {},
   "source": [
    "We paste some of the functions and commands we used in the notebook `session1a_RidgeCrossVal.ipynb` to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e52648",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_boston[column_names]\n",
    "y = target\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b0115",
   "metadata": {},
   "source": [
    "**Quick question: Do you see why the numpy arrays `X` and `y` have these dimensions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd91d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we continue with the preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled =scaler.fit_transform(X)\n",
    "test_share = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled,y,random_state=10,test_size=test_share)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73aa98",
   "metadata": {},
   "source": [
    "We paste some of the functions we used in part 1. In particular, these functions are meant have been used to\n",
    "   - plot correlation matrices,\n",
    "   - evaluate the $R^2$ value on training and test or validation set for a given predictive model,\n",
    "   - plot errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850abdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_true_predicted(y_train,y_test,model,figsize=(8,8)): # Plot correlation matrix of expected vs. \"measured\" median housing price for model\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(y_train,model.predicted_train)\n",
    "    plt.scatter(y_test, model.predicted_test)\n",
    "    maxx= max(np.max(model.predicted_train),np.max(model.predicted_test),np.max(y_train),np.max(y_test))\n",
    "    ax=plt.gca()\n",
    "    ax.set_ylim(0, 1.02*maxx)\n",
    "    ax.set_xlim(0, 1.02*maxx)\n",
    "    ax.legend([\"training data\",\"test data\"], loc=0)\n",
    "    ax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\n",
    "    ax.set(ylabel='predicted median housing price (in $1000)', xlabel='true median housing price (in $1000)')\n",
    "    ax.set(title=\"Correlation between true and predicted prices for model \"+str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45898c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prediction_train_test(model,X_train,X_test,y_train,y_test,plot=True):\n",
    "    model.score_train = model.score(X_train, y_train)\n",
    "    model.score_test = model.score(X_test, y_test)\n",
    "    model.predicted_test = model.predict(X_test)\n",
    "    model.predicted_train = model.predict(X_train)\n",
    "    print(\"R^2 value of model\",str(model),\"on the train set: %f\" % model.score_train)\n",
    "    print(\"R^2 value of model\",str(model),\"on the test set: %f\" % model.score_test)\n",
    "    if plot:\n",
    "        plot_correlation_true_predicted(y_train,y_test,model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d19d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting function for training und test errors for different model complexities:\n",
    "def plot_training_test(alphas,train_error,test_error):\n",
    "    plt.figure()\n",
    "    plt.plot(alphas,train_error)\n",
    "    plt.plot(alphas,test_error)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set(xlabel='alpha', ylabel='mean squared error')\n",
    "    ax.legend([\"training data\",\"test data\"], loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99204a7b-222e-45ba-8731-9ddf703f3c25",
   "metadata": {},
   "source": [
    "Furthermore, the next function wraps `GridSearchCV` for $k$-fold crossvalidation (with $k$=`cv`). See below for how it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_crossvalidation_model(model,X,y,parameters,cv=5,plot=True):\n",
    "    gridsearch = GridSearchCV(model,param_grid=parameters,scoring='r2',return_train_score=True,cv=cv) # cv\n",
    "    gridsearch.fit(X, y)\n",
    "    gridsearch.train_errors = gridsearch.cv_results_['mean_train_score']\n",
    "    gridsearch.test_errors  = gridsearch.cv_results_['mean_test_score']\n",
    "    if plot: # Plot training und test errors for different model complexities of ridge regression\n",
    "        plot_training_test(model,gridsearch.train_errors,gridsearch.test_errors)\n",
    "    return gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3086f63",
   "metadata": {},
   "source": [
    "### Lasso and Cross Validation\n",
    "\n",
    "Repeat previous experiments from `session1a_RidgeCrossVal.ipynb` about accuracy evaluation and 5-fold cross validation with Lasso instead of ridge regression. <br> \n",
    "\n",
    "First, we train Lasso for fixed $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec29756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "alpha_lasso = 1\n",
    "lasso = # train LASSO for fixed regularization parameter alpha=1\n",
    "lasso = eval_prediction_train_test(lasso,X_train,X_test,y_train,y_test)\n",
    "print(\"Number of features used for model \"+str(lasso)+\": {}\".format(np.sum(lasso.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834c5fd",
   "metadata": {},
   "source": [
    "Now, **run cross validation to optimize regularization parameter alpha of Lasso on the training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9760404",
   "metadata": {},
   "outputs": [],
   "source": [
    "## td logarithmically interpolated values between 10^(-5) and 10^(9)\n",
    "parameters = {'alpha':alphas}\n",
    "# Run cross validation to optimize regularization parameter alpha of Lasso\n",
    "gridsearch_lasso = \n",
    "lasso_optimized = \n",
    "print(\"Number of features used for model \"+str(lasso_optimized)+\": {}\".format(np.sum(lasso_optimized.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc9aec",
   "metadata": {},
   "source": [
    "We ran Lasso first for $\\alpha=1$, which leads to a model with fewer non-zero coefficients. However, we see that the resulting errors are likely larger, i.e., the $R^2$ values for test and training set are smaller than for ridge regression.\n",
    " \n",
    "Optimizing $\\alpha$ by cross validation leads to a model with more non-zero coefficeints, but better predictive properties (also improving on ridge regression in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8711ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Summarize results\n",
    "from sklearn.metrics import mean_squared_error\n",
    "results = {'Lasso': [lasso_optimized.score_test,mean_squared_error(lasso_optimized.predict(X_test), y_test)]}\n",
    "df_results = pd.DataFrame(data=results,index=['R2 on test set','Test MSE']) \n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58291fb6",
   "metadata": {},
   "source": [
    "To explore the interpretability of our models, we plot the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_ridge = gridsearch_crossvalidation_model(Ridge(),X_train,y_train,parameters,plot=False)\n",
    "ridge_optimized = gridsearch_ridge.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4314b",
   "metadata": {},
   "source": [
    "We plot coefficients for the two different models - Lasso and Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,14))\n",
    "plt.plot(lasso_optimized.coef_, '^', label=str(lasso_optimized))\n",
    "plt.plot(ridge_optimized.coef_, 'v', label=str(ridge_optimized))\n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.xticks(ticks=np.arange(0,11),labels=df_boston.columns[0:11])\n",
    "ax = plt.gca()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b18ca",
   "metadata": {},
   "source": [
    "So far, we have used only the $11$ original features to run all models (Ridge/Lasso). In the first notebook `session1a_RidgeCrossVal.ipynb`, we saw that enriching the features by preprocessing, e.g., through a polynomial map can improve the performance of linear models significantly.\n",
    "\n",
    "**In the next task, we apply the above models again to the the a set of predictor variables consisting of polynomials of degree of at most three of the original features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3780ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create regressor variables with polynomial features\n",
    "degree_polynomial = 3\n",
    "poly = \n",
    "\n",
    "X_train_poly = \n",
    "X_test_poly = \n",
    "\n",
    "# Rescale variables\n",
    "scaler = \n",
    "X_train_poly = \n",
    "X_test_poly = \n",
    "\n",
    "#X_poly      = poly.transform(X_scaled)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape)) \n",
    "print(\"X_train_poly.shape: {}\".format(X_train_poly.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a94962",
   "metadata": {},
   "source": [
    "We first run ridge regression for fixed alpha regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13334ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ridge = 1\n",
    "ridge = \n",
    "\n",
    "ridge = eval_prediction_train_test(ridge,X_train_poly,X_test_poly,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0450ae6",
   "metadata": {},
   "source": [
    "Run ridge regression for cross validated regularization parameter alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e00db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_ridge = gridsearch_crossvalidation_model(Ridge(),X_train_poly,y_train,parameters,plot=False)\n",
    "ridge_optimized = gridsearch_ridge.best_estimator_\n",
    "ridge_optimized = eval_prediction_train_test(ridge_optimized,X_train_poly,X_test_poly,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad1c47",
   "metadata": {},
   "source": [
    "**Note:** In the following cell, convergence warnings might arise. This is due to the fact that we do not have simple closed-from solution here, but need to run an iterative algorithm to minimize the Lasso objective. The optimizer is based on coordinate descent and run by `scikit-learn` in the backend. Potentially, a different algorithm optimization the same objective might here be useful.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run lasso for cross validated regularization parameter alpha\n",
    "gridsearch_lasso = gridsearch_crossvalidation_model(Lasso(max_iter=1000,warm_start=True),X_train_poly,y_train,parameters,plot=False)\n",
    "lasso_optimized = gridsearch_lasso.best_estimator_\n",
    "lasso_optimized = eval_prediction_train_test(lasso_optimized,X_train_poly,X_test_poly,y_train,y_test)\n",
    "print(\"Number of features used for model \"+str(lasso_optimized)+\": {}\".format(np.sum(lasso_optimized.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8bcd11",
   "metadata": {},
   "source": [
    "For Lasso regression with cross-validated regularization parameter $\\alpha$, we can now look at the _sparsity_, i.e., the number of non-zero coefficients in the coefficient vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars(lasso_optimized)\n",
    "lasso_optimized.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa653b4",
   "metadata": {},
   "source": [
    "We see that the this coefficient vector has a lot of zero entries and just few-nonzero ones. In particular, while the dimensionality of the vector is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92438839",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(lasso_optimized.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0a442",
   "metadata": {},
   "source": [
    "the number of non-zeros is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643147f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(lasso_optimized.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac300d0",
   "metadata": {},
   "source": [
    "Summarizing the performance of the different regression methods using polynomial features, we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize results for models trained on new features\n",
    "from sklearn.metrics import mean_squared_error\n",
    "results_poly = {'Ridge': [ridge_optimized.score_test, mean_squared_error(ridge_optimized.predict(X_test_poly), y_test)], \n",
    "           'Lasso': [lasso_optimized.score_test,mean_squared_error(lasso_optimized.predict(X_test_poly), y_test)]}\n",
    "df_results_poly = pd.DataFrame(data=results_poly,index=['R2 on test set','Test MSE']) \n",
    "print(df_results_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859eed0f",
   "metadata": {},
   "source": [
    "Comparing it with the $R^2$ values above, we see that working with polynomial features clearly improved the performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ec14b",
   "metadata": {},
   "source": [
    "Finally, we create a plot that visualizes the differences between the coefficients returned by the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b85f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot coefficients for different models\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.plot(lasso_optimized.coef_, '^', label=str(lasso_optimized))\n",
    "plt.plot(ridge_optimized.coef_, 'v', label=str(ridge_optimized))\n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "#plt.ylim(-25, 25)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "ax = plt.gca()\n",
    "#ax.set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64feb753",
   "metadata": {},
   "source": [
    "We observe that the coefficient vector returned by ridge regression has many small, but non-zero coefficients, whereas Lasso has few non-zero, but larger coefficients.\n",
    " \n",
    " \n",
    "Relating the coefficient index numbers with the original polynomial features created, we the see therefore that the sparse regression method (Lasso) given us further options to _interpret_ model:\n",
    " \n",
    "Each coefficient index corresponds to a certain monomial created from the 11 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d19bf",
   "metadata": {},
   "source": [
    "so that we can learn which (combinations of) features are able to create a good predictive model. The fewer non-zero coefficients we have, the \"simpler\" and \"easier to interpret\" the model becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb06f57",
   "metadata": {},
   "source": [
    "**Question: Can you identify which variables / variable combinations are the most significant in the Lasso model trained above for polynomial features?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add your code here ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:inmas-ml]",
   "language": "python",
   "name": "conda-env-inmas-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
