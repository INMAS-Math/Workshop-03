{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98dece3d",
   "metadata": {},
   "source": [
    "# Session 1a - Inmas Workshop Machine Learning Workshop, January 13-14, 2024\n",
    "\n",
    "Instructor: Christian Kuemmerle - kuemmerle@uncc.edu\n",
    "\n",
    "**This version of the notebook is more suitable for students with less experience in machine learning / who are less familiar with using Python for machine learning and/or the underlying concepts. It requires less coding/ Python fluency compared to the version ``session1a_RidgeCrossVal_advanced.ipynb``, but is similar in nature.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128e273-7134-4d77-ba31-454b5a25a5de",
   "metadata": {},
   "source": [
    "## Regularization and Ridge Regression, Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3903e27-ad9e-45f0-bc59-fb9209304928",
   "metadata": {},
   "source": [
    "### Case study with a simple one-dimensional function\n",
    "\n",
    "On the example of a simple sinusoidal one-dimensional function, we explore:\n",
    "   - the idea of a [training / test (or validation) set split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) of samples, and based on this, we evaluate the predictive power of \n",
    "   - [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html),\n",
    "   - [ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), which is an **$L_2$-regularized** linear model, \n",
    "   \n",
    "and furthermore, explore the effect of enriching the model by using [polynomial features](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd513e",
   "metadata": {},
   "source": [
    "First, we import the packages we need. As during the entire weekend, we will use the package [scikit-learn](https://scikit-learn.org/stable/).\n",
    "\n",
    "If you have not yet done so, you may go to the pre-work Jupyter notebook `session0a_test_environments.ipynb` available [here](https://webpages.charlotte.edu/~ckuemme1/teaching/machinelearningworkshop2023/00-preparation.html) and **execute the its first code** cell to install scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41fdfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d05a1",
   "metadata": {},
   "source": [
    "Importing `sklearn` should not result in an error after going through the pre-work notebook, however, if it does, please check out the `environment.yml` file available on the Github repository / shared OneDrive folder and and create a respective conda enviroment. Also note the [installation instructions](https://scikit-learn.org/stable/install.html) for your specific operating system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0514d9",
   "metadata": {},
   "source": [
    "### A simple one-dimensional function\n",
    "\n",
    "We define a function to sample $n$=`nr_samples` points $x_1,\\ldots,x_n$ uniformly distributed at random and define corresponding samples $y_i=f(x_i) + \\epsilon_i$ where $f(x)= \\sin(4 x) + x$ and $\\epsilon_i$ are i.i.d. standard normal random variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f261e",
   "metadata": {},
   "source": [
    "We start by visualizing the function $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02322c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = lambda x: np.sin(4 * x) + x # This is one convenient way to define simple, \"anonymous\" functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95004759",
   "metadata": {},
   "source": [
    "For more about lambda expressions to define anonymous functions, see also [here](https://realpython.com/python-lambda/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function f:\n",
    "line_xvalues = np.linspace(-3, 3, 1000).reshape(-1, 1) # generate 1d grid for x-axis samples\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(line_xvalues,fn(line_xvalues))\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_xlim(-8, 8)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c91651",
   "metadata": {},
   "source": [
    "We create a data set (consisting of $x$-coordinates in '$X$' and $y$-coordinates in '$y$') based on the above model, using 'nr_samples' samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44601020",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to create random samples from a wave-like signal\n",
    "def make_wave(n_samples=100):\n",
    "    rnd = np.random.RandomState(42) # this fixes the random seed (for reproducibility).\n",
    "    x = rnd.uniform(-3, 3, size=n_samples)\n",
    "    x = np.sort(x)\n",
    "    y_no_noise = (np.sin(4 * x) + x)\n",
    "    y = y_no_noise + rnd.normal(size=len(x))\n",
    "    return x.reshape(-1, 1), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4be6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Create data\n",
    "nr_samples = 50\n",
    "X, y = make_wave(n_samples=nr_samples)\n",
    "y = y.reshape((len(y), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dbf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X values:\", X.T,\"\\n y values:\", y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeff692",
   "metadata": {},
   "source": [
    "## Training data and test data sets\n",
    "\n",
    "We now use the method [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test#sklearn.model_selection.train_test_split) of scikit-learn to split the available samples into two random subsets, a training dataset and a test dataset. The share of samples in the training data is provided by the choice of 'train_share', i.e., if `train_share` $= 0.6$, then 60% of the samples are used in the training dataset.\n",
    " \n",
    "From the samples variable $X$, we create a [pandas](https://pandas.pydata.org/docs/) DataFrame dfX whose methods are sometimes more convenient to use. <br>\n",
    "**Complete the last line below by using train_test_split** for the DataFrame dfX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e22222",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_share = 0.6\n",
    "dfX = pd.DataFrame(X)\n",
    "X_train, X_test, y_train, y_test = # split data into a training data set of size ('train_share'*nr_samples)\n",
    "# and test data set of size (1-'train_share')*nr_samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae310a61",
   "metadata": {},
   "source": [
    "To see what happened, we print the x-values of both training and test dataset. As we work with `pandas` DataFrame data structures, we can simply type the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2249292",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7186bbb6",
   "metadata": {},
   "source": [
    "The first columns above contain the indices of the respective data points. <br> \n",
    "We can now easily obtain a vector of indices corresponding to the training and test set, respectively. We define the corresponding vectors 'id_train' and 'id_test'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212265f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train = X_train.index # obtain index of training set\n",
    "id_test = X_test.index # obtain index of test set\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff3206",
   "metadata": {},
   "source": [
    "Next, **please convert the pandas.DataFrame of X_train and X_test to a multi-dimensional array in numpy**. You may use [this](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de545850",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train\n",
    "X_test_ = X_test\n",
    "### Add your code below ###\n",
    "X_train = # convert X_train_ to numpy.ndarray X_train\n",
    "X_test = # convert X_test_ to numpy.ndarray X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6818ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The samples can be visualized as follows.\n",
    "# %% Plot training data\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(X_train,y_train,'o',c='blue')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.legend([\"training data\"],loc=0,fontsize=8)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e1a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot training data and test data\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(X_train,y_train,'o',c='blue')\n",
    "plt.plot(X_test,y_test,'o',c='red')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.legend([\"training data\",\"test data\"],loc=0,fontsize=8)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b40543",
   "metadata": {},
   "source": [
    "We now train a linear regression model on the samples of the training set (without any modification on the features). This corresponds to a linear model in the standard coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38769f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Fit linear regression model to training data\n",
    "lr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2cb9bc",
   "metadata": {},
   "source": [
    "**What are the _coefficient_ and the _intercept_ of the fitted linear regression model?**\n",
    "<br> Hint: Use `vars` or look up in the documentation of `sklearn.linear_model.LinearRegression` how to access these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add your code below ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec29d7c",
   "metadata": {},
   "source": [
    "### Generation of Polynomial Features\n",
    " \n",
    "Based on the wave-like behavior of the function $f$ that is underlying the data generation process, \n",
    "we would like to have larger expressive power of the models that we use.\n",
    "For this reason we use an appropriate method of sklearn's [preprocessing](https://scikit-learn.org/stable/modules/classes.html?highlight=preprocessing#module-sklearn.preprocessing) to transform the $x$-samples to a vector of monomials, and run linear regression on these \"enhanced\" features derived from the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98798b47",
   "metadata": {},
   "source": [
    "Run linear regression with polynomial features of degree `'degree_poly'` (which can be chosen as 10 here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43a2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_poly = 10;\n",
    "poly = PolynomialFeatures(degree_poly) # define polynomial data model\n",
    "X_trainpoly = poly.fit_transform(X_train) # obtain coordinates of polynomial features of training set\n",
    "X_testpoly = poly.transform(X_test) # obtain coordinates of polynomial features of training set\n",
    "polylr = LinearRegression().fit(X_trainpoly,y_train) # perform linear regression with polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d126314",
   "metadata": {},
   "source": [
    "Now, **we run ridge regression on the polynomial features for a fixed regularization parameter** $\\alpha=\\text{alphaval}$. Please note the options of [linear_models](https://scikit-learn.org/stable/modules/linear_model.html#linear-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4568ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaval = 10 # ridge regression parameter alpha\n",
    "polyridge =  # perform ridge regression with polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609d03e",
   "metadata": {},
   "source": [
    "**Print, analogously to the linear regression model above with exclusively linear features above, the learned model parameters in this case as well. <br> <br> What do you notice?** <br>\n",
    "Play with different values of `alphaval` and see how the coefficients behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7052a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add your code below ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf619c37",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We want to start quantifying the predictive accuracy of the different models. To that end, we evaluate the mean squared errors as well as the resulting $R^2$ _coefficient of determination_ (see this for an [explanation](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score)).\n",
    "\n",
    "**Compute the $R^2$ for the three considered models, for both training and test set, as well as the _mean squared errors_ for these, at the appropriate position of `trainerrors_list`, `testerrors_list`, `r2_train_list` and `r2_test_list`**.\n",
    "\n",
    "Hint: For the three models, `lr`, `polylr` and `polyridge`, you can use e.g. `dir(polyridge)` to see a list of methods available for e.g. evaluating the model on your training or test data and for computing the $R^2$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2439d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trainerrors = ['Training error of '+str(lr)+':','Training error of '+str(lr)+', poly. feat.:',\n",
    "                    'Training error of '+str(polyridge)+', poly. feat.:']\n",
    "print(text_trainerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainerrors_list =     # add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_testerrors = ['Test error of '+str(lr)+':','Test error of '+str(lr)+', poly. feat.:','Test error of '+str(polyridge)+', poly. feat.:']\n",
    "print(text_testerrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678515fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "testerrors_list =      # add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_r2 = ['R2 (train) of '+str(lr)+':','R2 (train) of '+str(lr)+', poly. feat.:','R2 (train) of '+str(polyridge)+', poly. feat.:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train_list =        # add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567977f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_r2_test = ['R2 (test) of '+str(lr)+':','R2 (test) of '+str(lr)+', poly. feat.:','R2 (test) of '+str(polyridge)+', poly. feat.:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adaf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_test_list =         # add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62561fe9",
   "metadata": {},
   "source": [
    "To obtain simple, formatted output, it is convenient to create a pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame) that includes the text as a row description and the quantities as entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "print(pd.DataFrame(trainerrors_list, index=text_trainerrors, columns=['']))\n",
    "print(pd.DataFrame(r2_train_list, index=text_r2, columns=['']))\n",
    "\n",
    "print(pd.DataFrame(testerrors_list, index=text_testerrors, columns=['']))\n",
    "print(pd.DataFrame(r2_test_list, index=text_r2_test, columns=['']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d492d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We summarize the resulting curves in a plot.\n",
    "# %% Plot summary\n",
    "line = np.linspace(-8, 8, 1000).reshape(-1, 1)\n",
    "line_poly = poly.transform(line)\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(X_train,y_train,'o',c='blue')\n",
    "plt.plot(X_test,y_test,'o',c='red')\n",
    "plt.plot(line, lr.predict(line))\n",
    "plt.plot(line, polylr.predict(line_poly))\n",
    "plt.plot(line, polyridge.predict(line_poly))\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.set_ylim(-6, 6)\n",
    "ax.legend([\"training data\",\"test data\",\"linear\",\"lin., poly. features\",\"ridge, alpha=\"+str(alphaval)], loc=0,fontsize=10)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c29911",
   "metadata": {},
   "source": [
    "**How do you interpret the accuracy scores above in view of this plot?**\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6b438",
   "metadata": {},
   "source": [
    "## Exploring trade-offs of model complexity\n",
    "\n",
    "## Linear regression on polynomial features of different degree\n",
    "\n",
    "We now consider the resulting errors of applying linear regression with polynomial features of different degrees in order to explore the behavior of training and test errors for different model complexities.\n",
    " \n",
    "To this end, we write a snippet of code that creates two [numpy.ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray) `train_errors` and `test_errors` that contain the mean squared errors on both training and test set that are obtained by running linear regression on polynomial features of degree between 1 and 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d0f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Explore trade-off of model complexity\n",
    "degree_range = range(1,30) # contains all integers between 1 and 30\n",
    "train_errors = np.zeros((len(degree_range), ))\n",
    "test_errors = np.zeros((len(degree_range), ))\n",
    "for k, degree in enumerate(degree_range):\n",
    "    poly =  # define polynomial data model\n",
    "    X_trainpoly =  # obtain coordinates of polynomial features of training set\n",
    "    X_testpoly = # obtain coordinates of polynomial features of training set\n",
    "    polylr = # perform linear regression with polynomial features\n",
    "    train_errors[k] = # compute Mean Squared Error on training set\n",
    "    test_errors[k] =  # compute Mean Squared Error on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9abca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c669314",
   "metadata": {},
   "source": [
    "Using this, we create a plot containing training and test errors on the $y$-axis and the degree of the polynomial model on the $x$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48684dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot training and test errors for different model complexities\n",
    "plt.figure()\n",
    "plt.plot(degree_range,train_errors)\n",
    "plt.plot(degree_range,test_errors)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.legend([\"training data\",\"test data\"], loc=0)\n",
    "ax.set(xlabel='degree of polynomial', ylabel='mean squared error',\n",
    "       title='Linear regression with polynomial features of different degree');\n",
    "\n",
    "print(\"Degree resulting in smallest error on test data: %f\" % (np.argmin(test_errors)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f1c96",
   "metadata": {},
   "source": [
    "We observe that for growing model complexity, the training error *decreases*, whereas the test error actually *increases*.\n",
    " \n",
    "**Can you explain this behavior?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2cab6b",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "Since the goal of a supervised machine learning model/algorithm is to attain high accuracy / small errors for a dataset drawn from a similar distribution as the training set (the test set), it is of interest to find good \"hyperparameters\" of the model that choose the \"best\" model among a class of candidate models. A typical workflow would look like this:\n",
    "\n",
    "<img src=https://scikit-learn.org/stable/_images/grid_search_workflow.png alt=\"image info\" width=\"400\" />\n",
    "\n",
    "## Ridge regression on polynomial features with different regularization parameters\n",
    " \n",
    "Next, we focus on ridge regression on features with fixed degree, but using different regularization parameters (between $\\alpha=10^{-5}$ and $\\alpha=10^{9}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.linalg import LinAlgWarning \n",
    "warnings.filterwarnings(\"ignore\",category= LinAlgWarning, module='sklearn') # suppresses a warning that might come up due to ill-conditioning of linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843212a",
   "metadata": {},
   "source": [
    "In the following, we fix the polynomial degree to 11. For such polynomial features, **write code that**\n",
    "   - **uses the available training-test set split to run a [hyperparameter optimization](https://scikit-learn.org/stable/modules/grid_search.html#tuning-the-hyper-parameters-of-an-estimator) for ridge regression over $60$ different values of $\\alpha$, logarithmically spaced between $10^{-5}$ and $10^{9}$. Relevant methods you might want to use are _PredefinedSplit_ and _GridsearchCV_.**\n",
    "   - **Eventually, extract the test and training errors into the numpy.ndarrays `train_errors_ridge` and `test_errors_ridge`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the polynomial features\n",
    "degree = 11\n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a38a6",
   "metadata": {},
   "source": [
    "We first focus on a (simpler) hyperparameter optimization over the already existing training-test set split (with folds etc.). Strictly speaking, this is not a good practice, since we use information from the test set to optimize a parameter in the learned model. We will be more careful about this when working with a \"real\" data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fold = np.zeros((nr_samples, ))\n",
    "test_fold[id_train] = -1\n",
    "test_fold[id_test] = 0\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "cv = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de10a5",
   "metadata": {},
   "source": [
    "Using this existing split, we use hyperparameter optimization for ridge regression to optimize the parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba45c1d-49e5-4f66-9ca4-c6908d6017f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas =  # create vector of logarithmically interpolated values between 10^(-5) and 10^(9) (hint: see np.logspace)\n",
    "parameters = {'alpha':alphas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2da68-cde4-454e-a703-ad07505ab34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch =  # (hint: see the GridSearchCV method)\n",
    "\n",
    "train_errors_ridge = \n",
    "test_errors_ridge = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting function for training and test errors for different model complexities:\n",
    "def plot_train_test(alphas,train_error,test_error):\n",
    "    plt.figure()\n",
    "    plt.plot(alphas,train_errors_ridge)\n",
    "    plt.plot(alphas,test_errors_ridge)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set(xlabel='alpha', ylabel='mean squared error',\n",
    "           title='Ridge regession for different model complexities')\n",
    "    ax.legend([\"training data\",\"test data\"], loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4279081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot resulting train/test errors:\n",
    "plot_train_test(alphas,train_errors_ridge,test_errors_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa6a4c",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "Alternatively, we also perform [k-fold cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html?highlight=kfold#sklearn.model_selection.KFold) with $k=5$ (see also [here](https://scikit-learn.org/stable/modules/cross_validation.html#k-fold)) of the same parameter $\\alpha$ on the training set only.\n",
    "\n",
    "In any realistic machine learning problem, this method would be much preferred over what we did above: During the process of determining the best hyperparameter $\\alpha$, the test set is not touched at all - it would be only considered at the end after determining the hyperparameter to \"test\" the accuracy. In this way, it is more likely that we have learned to understand the actual data distribution instead of potentially overfitting our model to the particular, finite dataset at hand.\n",
    "\n",
    "<img src=https://scikit-learn.org/stable/_images/grid_search_cross_validation.png alt=\"image info\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7489e18",
   "metadata": {},
   "source": [
    "**Find an optimal ridge regression model by performing 5-fold cross validation on the training set (polynomial features), and report the average errors on training sets and validation sets,**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_kfold = GridSearchCV(Ridge(),param_grid=parameters,\n",
    "                                scoring='neg_mean_squared_error',\n",
    "                                return_train_score=True,cv=5)\n",
    "X_trainpoly = poly.transform(X_train)\n",
    "gridsearch_kfold.fit(X_trainpoly, y_train)\n",
    "train_errors_ridge_kfold = -gridsearch_kfold.cv_results_['mean_train_score']\n",
    "validation_errors_ridge_kfold = -gridsearch_kfold.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we plot the resulting train/validation errors:\n",
    "plot_train_test(alphas,train_errors_ridge_kfold,validation_errors_ridge_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dec03a",
   "metadata": {},
   "source": [
    "Comparing the last two plots, we see that for this data set, the two different cross-validation strategies result in basically the same behavior.\n",
    "\n",
    "Finally, let's plot the obtained function stemming from the \"optimal\" ridge regression model (optimized regularization parameter $\\alpha$) compared to the ridge regression model that we obtain if we choose $\\alpha = 10^{-5}$ or $\\alpha = 10^{9}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d05853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot optimal, under-fitted and over-fitted model\n",
    "alpha_opt = gridsearch.best_params_['alpha']\n",
    "alpha_overfit = alphas[-1]\n",
    "alpha_underfit = alphas[0]\n",
    "X_trainpoly0 = poly.fit_transform(X_train) # obtain coordinates of polynomial features of training set\n",
    "line_poly = poly.transform(line)\n",
    "rd = Ridge(alpha=alpha_opt).fit(X_trainpoly0,y_train)\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(X_train,y_train,'o',c='blue')\n",
    "plt.plot(X_test,y_test,'o',c='red')\n",
    "plt.plot(line,rd.predict(line_poly))\n",
    "rd = Ridge(alpha=alpha_overfit).fit(X_trainpoly0,y_train)\n",
    "plt.plot(line,rd.predict(line_poly))\n",
    "rd = Ridge(alpha=alpha_underfit).fit(X_trainpoly0,y_train)\n",
    "plt.plot(line,rd.predict(line_poly))\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.legend([\"training data\",\"test data\",\"optimized alpha\",\"alpha=\"+str(alpha_overfit),\"alpha=\"+str(alpha_underfit)], loc=0)\n",
    "\n",
    "print(\"'Optimal' regularization parameter alpha (i.e., resulting in smallest test error): %f\" % alpha_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba8a75",
   "metadata": {},
   "source": [
    "We observe that for the smallest regularization parameter $\\alpha = 10^{-5}$ (which almost corresponds to a linear regression model without regularization), we can interpolate the training data points very well, at the cost of a large error on the test data points.\n",
    " \n",
    "On the other hand, for large regularization parameters such as $\\alpha = 10^9$, we obtain a \"simple\" model which interpolates the training data less well, with a comparable error on the test set.\n",
    " \n",
    "The minimal error on the test data will be achieved for a value of $\\alpha$ that is in between."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb6132",
   "metadata": {},
   "source": [
    "## Regression on the [Boston housing dataset](https://www.openml.org/d/531)\n",
    "## Case study with a real data set (part 1)\n",
    "\n",
    "In the following, we study and compare the performance of different regression methods applied to a data set with 11 regressors and one target variable. The dataset is a [housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) about the Boston area from 1978.\n",
    "\n",
    " For this data set, we cover the\n",
    "   - visualization of the data,\n",
    "   - preprocessing of the data,\n",
    "   - differences between learning algorithms, in particular, between ridge regression and sparse regression (Lasso),\n",
    "   - cross-validation of an algorithmic hyperparameter.\n",
    "\n",
    "We will complete first two tasks in part 1, and complete the last two tasks in part 2 of this session.\n",
    "\n",
    "## Understanding the dataset\n",
    "\n",
    "First, we load the data. We then create a [pandas](https://pandas.pydata.org/docs/) DataFrame from the data as it helps to visualize it in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb673a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO']\n",
    "df_boston = pd.DataFrame(data[:,0:11],columns = column_names)\n",
    "df_boston.assign(MEDV=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some descriptive statistics of dataset:\n",
    "df_boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc190f38",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Create histrogram of the target variable, the median house prices, using [matplotlib](https://matplotlib.org):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# %% Plot histogram of median house prices in dataset\n",
    "plt.figure()\n",
    "plt.hist(target)\n",
    "ax1 = plt.gca()\n",
    "ax1.set(xlabel='median housing price (in $1000)', ylabel='count',\n",
    "       title='Histogram of Boston area housing price medians')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb93130",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The regressor variables cover varying scales: While the variable 'CRIM', the per capita crime rate, ranges between 0.006320 and 88.97, the variable 'NOX' (nitric oxides concentration in parts per 10 million) has a range between 0.385 and 0.871. This will have implications on the magnitudes of the coefficients in the linear models that are used by the learning algorithms we consider.\n",
    "\n",
    "In order to mitigate the issue of these varying scales, we preprocess the data by **scaling** all feature/regressor variables to lie between 0 and 1. <br>\n",
    "More information about preprocessing methods (scaling and others) can be found [here](https://scikit-learn.org/stable/modules/preprocessing.html) and [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py).\n",
    "\n",
    "**Exercise:** Repeat experiments below _without_ this preprocessing. How does the performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83288bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% rescale regressor coordinates (improves regression performance by levelling the regression coefficient sizes)\n",
    "# create numpy arrays representing features and target values\n",
    "X = \n",
    "y = \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler =  # define scaling object\n",
    "X_scaled = # apply scaling to X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e73cd0",
   "metadata": {},
   "source": [
    "## Data split for experimental setup\n",
    "\n",
    "We split our data set into two parts, a training and a test set. The test set contains the a share of 'test_share' of the total data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_share = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled,y,random_state=10,test_size=test_share)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c20a32",
   "metadata": {},
   "source": [
    "## Run learning algorithms\n",
    "\n",
    "### Ridge Regression for fixed regualarization\n",
    "\n",
    "We run ridge regression for a fixed regularization parameter on the training set. By defining two functions for evaluating the performance of the model and for the visualization of a correlation between predicted target variable and true target variable, we not only achieve what we want for the ridge regression model, but can easily reuse the code for other learning algorithms below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Set up ridge regression model\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha_ridge = 100\n",
    "ridge = Ridge(alpha=alpha_ridge,fit_intercept=True)\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbcd4f",
   "metadata": {},
   "source": [
    "Compute prediction scores for training and test data for model, plot correlation matrix of expected vs. \"measured\" median housing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_true_predicted(y_train,y_test,model,figsize=(8,8)): # Plot correlation matrix of expected vs. \"measured\" median housing price for model\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(y_train,model.predicted_train)\n",
    "    plt.scatter(y_test, model.predicted_test)\n",
    "    maxx= max(np.max(model.predicted_train),np.max(model.predicted_test),np.max(y_train),np.max(y_test))\n",
    "    ax=plt.gca()\n",
    "    ax.set_ylim(0, 1.02*maxx)\n",
    "    ax.set_xlim(0, 1.02*maxx)\n",
    "    ax.legend([\"training data\",\"test data\"], loc=0)\n",
    "    ax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\n",
    "    ax.set(ylabel='predicted median housing price (in $1000)', xlabel='true median housing price (in $1000)')\n",
    "    ax.set(title=\"Correlation between true and predicted prices for model \"+str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff202078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prediction_train_test(model,X_train,X_test,y_train,y_test,plot=True):\n",
    "    model.score_train = model.score(X_train, y_train)\n",
    "    model.score_test = model.score(X_test, y_test)\n",
    "    model.predicted_test = model.predict(X_test)\n",
    "    model.predicted_train = model.predict(X_train)\n",
    "    print(\"R^2 value of model\",str(model),\"on the train set: %f\" % model.score_train)\n",
    "    print(\"R^2 value of model\",str(model),\"on the test set: %f\" % model.score_test)\n",
    "    if plot:\n",
    "        plot_correlation_true_predicted(y_train,y_test,model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ccc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = eval_prediction_train_test(ridge,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacfa9d",
   "metadata": {},
   "source": [
    "In this plot, the closeness of data points to the main diagonal visualized the accuarcy of the prediction. It has some similarities to a plot of the confusion matrix for classification problems, which we will encounter later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faff5c5",
   "metadata": {},
   "source": [
    "### Ridge regression with cross-validated regularization  paramter (hyperparameter optimization via grid search)\n",
    "\n",
    "Next, we optimize the regularization parameter alpha of ridge regression. We use a grid of logarithmically spaced values between $10^{-5}$ and $10^{-9}$, and a 5-fold [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Use cross validation to optimize regularization parameter alpha of ridge regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "alphas=np.logspace(-5,9,num=60) # create vector of logarithmically interpolated values between 10^(-5) and 10^(9)\n",
    "parameters = {'alpha':alphas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(Ridge(),param_grid=parameters,scoring='r2',return_train_score=True,cv=5) # cv\n",
    "gridsearch.fit(X, y)\n",
    "gridsearch.train_errors = gridsearch.cv_results_['mean_train_score']\n",
    "gridsearch.test_errors  = gridsearch.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c0000",
   "metadata": {},
   "source": [
    "Next, we plot the training against the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_test(model,parameters,train_errors,test_errors): # Plot training and test errors for different model complexities of ridge regression\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(list(parameters.values())[0],train_errors)\n",
    "    plt.plot(list(parameters.values())[0],test_errors)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set(xlabel='alpha', ylabel='R2 value',\n",
    "           title=str(model)+' for different model complexities')\n",
    "    ax.set_box_aspect(1)\n",
    "    ax.legend([\"training data\",\"test data\"], loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_test(Ridge(),parameters,gridsearch.train_errors,gridsearch.test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642286dd",
   "metadata": {},
   "source": [
    "Now crossvalidate only on training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ccf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_train = GridSearchCV(Ridge(),param_grid=parameters,scoring='r2',return_train_score=True,cv=5)\n",
    "gridsearch_train.fit(X_train,y_train)\n",
    "ridge_optimized = gridsearch_train.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d13d9",
   "metadata": {},
   "source": [
    "We plot the true prices vs. the prices predicted by the \"optimized\" ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca18818",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_optimized = eval_prediction_train_test(ridge_optimized,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59517ba",
   "metadata": {},
   "source": [
    "In the first plot, we see the $R^2$ value (which can be thought of as a value that is negatively correlated to the mean squared error, or empirical risk in our case). The regularization parameter $\\alpha$ to be used in our predicive model is the one that maximizes the $R^2$ value on the test set (in this context, the term *validation set* is more appropriate)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:inmas-ml]",
   "language": "python",
   "name": "conda-env-inmas-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
