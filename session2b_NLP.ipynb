{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5032f84-76f0-4c06-9251-f9973cc0e791",
   "metadata": {},
   "source": [
    "# Session 2b - Inmas Workshop Machine Learning Workshop, January 13-14, 2024\n",
    "\n",
    "Instructor: Christian Kuemmerle - kuemmerle@uncc.edu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1434d699-2af9-4274-a8f6-a56be78dcc7a",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "## Introduction to [Sentiment Analysis with VADER](https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/)\n",
    "\n",
    "[VADER (Valence Aware Dictionary and sEntiment Reasoner)](https://ojs.aaai.org/index.php/ICWSM/article/view/14550), proposed by C. Hutto and E. Glibert in 2014, uses lexicons and rules to evaluate the sentiment expressed in text. It is particularly useful in the context of evaluating sentiments in social media. Using VADER in Python is surprisingly simple. Below you can apply VADER to get an idea of how NLP can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aa40dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "try: \n",
    "    import vaderSentiment\n",
    "except ModuleNotFoundError:\n",
    "    !conda install --yes -c conda-forge vadersentiment\n",
    "    import vaderSentiment\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96f9bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print sentiments of sentences (code from GeeksforGeeks)\n",
    "def sentiment_scores(sentence):\n",
    " \n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    " \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # object gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "     \n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    " \n",
    "    print(\"Sentence Overall Rated As\", end = \" \")\n",
    " \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        print(\"Positive\")\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        print(\"Negative\")\n",
    " \n",
    "    else :\n",
    "        print(\"Neutral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3afd5b",
   "metadata": {},
   "source": [
    "Below we give a couple of examples to illustrate how just a few words can alter the sentiment of a sentence as measured by VADER. The first example is entirely neutral, while in the second example only one word, \"exciting,\" is added, but it changes the sentiment significantly. Learning things isn't necessarily good or bad, but learning exciting things is usually good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90bac597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_scores(\"I have learned things from INMAS workshops!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d091c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores(\"I have learned exciting things from INMAS workshops!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7b07e",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Try evaluating the sentiment of your own sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcfd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add your own sentence inside the parentheses - remember syntax for strings\n",
    "sentiment_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd177a1",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "How might other machine learning algorithms we've seen in this workshop be coupled with sentiment analysis to learn text-based datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781887b",
   "metadata": {},
   "source": [
    "## Classification of [Newsgroups dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) Text\n",
    "\n",
    "We now consider a text-based data set, which is based on e-mails. The e-mails have classified into 20 categories.\n",
    "\n",
    "The task is be to predict the categories of unseen e-mails based on a the knowledge of a set of already classified e-mails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fab406",
   "metadata": {},
   "source": [
    "### Understanding the data set\n",
    "\n",
    "We first load the data set and inspect its description. In this case, a seperation into training and test data is already provided (differentiated by the variable 'subset')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4eb5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e5d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= fetch_20newsgroups(subset=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4efbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "|details-start|\n",
      "**Usage**\n",
      "|details-split|\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "|details-end|\n",
      "\n",
      "|details-start|\n",
      "**Converting text to vectors**\n",
      "|details-split|\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "|details-end|\n",
      "\n",
      "|details-start|\n",
      "**Filtering text for more realistic training**\n",
      "|details-split|\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = vectorizer.get_feature_names_out()\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try the\n",
      ":ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "example with and without the `remove` option to compare the results.\n",
      "|details-end|\n",
      "\n",
      ".. topic:: Data Considerations\n",
      "\n",
      "  The Cleveland Indians is a major league baseball team based in Cleveland,\n",
      "  Ohio, USA. In December 2020, it was reported that \"After several months of\n",
      "  discussion sparked by the death of George Floyd and a national reckoning over\n",
      "  race and colonialism, the Cleveland Indians have decided to change their\n",
      "  name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
      "  its informal nickname -- the Tribe -- its new team name.\" \"It's not going to\n",
      "  be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
      "  American-themed name.\"\n",
      "\n",
      "  https://www.mlb.com/news/cleveland-indians-team-name-change\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  - When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "    should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
      "    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "    lower because it is more realistic.\n",
      "  - This text dataset contains data which may be inappropriate for certain NLP\n",
      "    applications. An example is listed in the \"Data Considerations\" section\n",
      "    above. The challenge with using current text datasets in NLP for tasks such\n",
      "    as sentence completion, clustering, and other applications is that text\n",
      "    that is culturally biased and inflammatory will propagate biases. This\n",
      "    should be taken into consideration when using the dataset, reviewing the\n",
      "    output, and the bias should be documented.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train.DESCR) #prints a description of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9147261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7\n",
      "Category:  rec.autos \n",
      "\n",
      "From: irwin@cmptrc.lonestar.org (Irwin Arnstein)\n",
      "Subject: Re: Recommendation on Duc\n",
      "Summary: What's it worth?\n",
      "Distribution: usa\n",
      "Expires: Sat, 1 May 1993 05:00:00 GMT\n",
      "Organization: CompuTrac Inc., Richardson TX\n",
      "Keywords: Ducati, GTS, How much? \n",
      "Lines: 13\n",
      "\n",
      "I have a line on a Ducati 900GTS 1978 model with 17k on the clock.  Runs\n",
      "very well, paint is the bronze/brown/orange faded out, leaks a bit of oil\n",
      "and pops out of 1st with hard accel.  The shop will fix trans and oil \n",
      "leak.  They sold the bike to the 1 and only owner.  They want $3495, and\n",
      "I am thinking more like $3K.  Any opinions out there?  Please email me.\n",
      "Thanks.  It would be a nice stable mate to the Beemer.  Then I'll get\n",
      "a jap bike and call myself Axis Motors!\n",
      "\n",
      "-- \n",
      "-----------------------------------------------------------------------\n",
      "\"Tuba\" (Irwin)      \"I honk therefore I am\"     CompuTrac-Richardson,Tx\n",
      "irwin@cmptrc.lonestar.org    DoD #0826          (R75/6)\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "8\n",
      "Category:  rec.motorcycles \n",
      "\n",
      "From: johnc@crsa.bu.edu (John Collins)\n",
      "Subject: Problem with MIT-SHM\n",
      "Organization: Boston University\n",
      "Lines: 27\n",
      "\n",
      "I am trying to write an image display program that uses\n",
      "the MIT shared memory extension.  The shared memory segment\n",
      "gets allocated and attached to the process with no problem.\n",
      "But the program crashes at the first call to XShmPutImage,\n",
      "with the following message:\n",
      "\n",
      "X Error of failed request:  BadShmSeg (invalid shared segment parameter)\n",
      "  Major opcode of failed request:  133 (MIT-SHM)\n",
      "  Minor opcode of failed request:  3 (X_ShmPutImage)\n",
      "  Segment id in failed request 0x0\n",
      "  Serial number of failed request:  741\n",
      "  Current serial number in output stream:  742\n",
      "\n",
      "Like I said, I did error checking on all the calls to shmget\n",
      "and shmat that are necessary to create the shared memory\n",
      "segment, as well as checking XShmAttach.  There are no\n",
      "problems.\n",
      "\n",
      "If anybody has had the same problem or has used MIT-SHM without\n",
      "having the same problem, please let me know.\n",
      "\n",
      "By the way, I am running OpenWindows 3.0 on a Sun Sparc2.\n",
      "\n",
      "Thanks in advance--\n",
      "John C.\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "Category:  comp.windows.x\n"
     ]
    }
   ],
   "source": [
    "# To see what the e-mails look like, we just display a few of them, as well as the corresponding category:\n",
    "print(train.data[0])\n",
    "print(train.target[0])\n",
    "print(\"Category: \",train.target_names[train.target[0]],\"\\n\")\n",
    "\n",
    "print(train.data[10])\n",
    "print(train.target[10])\n",
    "print(\"Category: \",train.target_names[train.target[10]],\"\\n\")\n",
    "\n",
    "print(train.data[50])\n",
    "print(train.target[50])\n",
    "print(\"Category: \",train.target_names[train.target[50]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68e4b3",
   "metadata": {},
   "source": [
    "The available categories are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafc49e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfbd61",
   "metadata": {},
   "source": [
    "We can see above that data contains full e-mails that contain header and (sometimes) footer information. Since we want to assess the behavior of methods based on text only, avoiding the additional information of metadata, we reimport the data using the respective option.\n",
    "\n",
    "Furthermore, we select just a subset of the emails in 12 categories (instead of all 20) in order to speed up computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af04668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_allcat = train\n",
    "categories =  ['alt.atheism','talk.religion.misc','comp.graphics','sci.space',\n",
    "               'comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware',\n",
    "               'talk.politics.misc','sci.med','rec.autos','sci.electronics','rec.motorcycles']\n",
    "\n",
    "train= fetch_20newsgroups(subset=\"train\",remove = ('headers', 'footers'),categories=categories)\n",
    "test= fetch_20newsgroups(subset=\"test\",remove = ('headers', 'footers'),categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999250e0",
   "metadata": {},
   "source": [
    "We proceed to obtain some understanding of the data set: We extract the frequency of each category, first for the training set, then for the test set. Also, we obtain the memory size of the data we process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507817a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "Class frequencies in training set:  [480 584 591 590 578 594 598 591 594 593 465 377]\n",
      "Class frequencies in test set:  [480 584 591 590 578 594 598 591 594 593 465 377]\n",
      "Size of text in training set (all categories, w/ headers & footers): 22.055 MB\n",
      "Size of text in training set: 9.900 MB\n",
      "Size of text in test set: 6.241 MB\n"
     ]
    }
   ],
   "source": [
    "classe, frequency_train = np.unique(train.target, return_counts=True)\n",
    "print(classe)\n",
    "print(\"Class frequencies in training set: \",frequency_train)\n",
    "_, frequency_test = np.unique(test.target, return_counts=True)\n",
    "print(\"Class frequencies in test set: \",frequency_train)\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "print(\"Size of text in training set (all categories, w/ headers & footers): %0.3f MB\" % size_mb(train_allcat.data))\n",
    "print(\"Size of text in training set: %0.3f MB\" % size_mb(train.data))\n",
    "print(\"Size of text in test set: %0.3f MB\" % size_mb(test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373bb50",
   "metadata": {},
   "source": [
    "Apparently, the number of the samples is quite balanced across the categories (with a considerable smaller number only of samples except for the last category).\n",
    "The order of the categories is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae82bab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ebdc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alt.atheism', 'talk.politics.misc', 'talk.religion.misc')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the category with the smallest number of samples in the training set (among the considered ones) are:\n",
    "train.target_names[0],train.target_names[10],train.target_names[11]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929ab64",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "#### Count Vectorizer\n",
    "\n",
    "We now extract **features** which we can use for learning algorithms. We choose the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer) first, using a bound of $2^{14}$ features to be used - this is done to make the computations below faster. Ideally, one would increase this bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1779752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "op = OptionParser()\n",
    "argv = []\n",
    "sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7c700f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a count vectorizer\n",
      "done in 1.241475s at 7.974MB/s\n",
      "n_samples: 6635, n_features: 16384\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features from the training data using a count vectorizer\")\n",
    "t0 = time()\n",
    "countvec = CountVectorizer(stop_words='english',max_features=2**14)\n",
    "X_train = countvec.fit_transform(train.data)\n",
    "X_test  = countvec.transform(test.data) # Extracting features from the test data using the same vectorizer\n",
    "duration = time() - t0\n",
    "# check computational effort to compute the features\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, size_mb(train.data) / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b023691",
   "metadata": {},
   "source": [
    "We create a pandas DataFrame in order to get an impression about the created dictionary and feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ee4e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['00', '000', '0001', '001', '0049', '00pm', '01', '02', '020', '024246',\n",
      "       ...\n",
      "       'zterm', 'zu', 'zuo', 'zv', 'zvm', 'zw', 'zx', 'zy', 'zyxel', 'zz'],\n",
      "      dtype='object', length=16384)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>001</th>\n",
       "      <th>0049</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>020</th>\n",
       "      <th>024246</th>\n",
       "      <th>...</th>\n",
       "      <th>zterm</th>\n",
       "      <th>zu</th>\n",
       "      <th>zuo</th>\n",
       "      <th>zv</th>\n",
       "      <th>zvm</th>\n",
       "      <th>zw</th>\n",
       "      <th>zx</th>\n",
       "      <th>zy</th>\n",
       "      <th>zyxel</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6630</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6631</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6632</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6634</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6635 rows × 16384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  0001  001  0049  00pm  01  02  020  024246  ...  zterm  zu  \\\n",
       "0      0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "1      0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "2      0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "3      0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "4      0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "...   ..  ...   ...  ...   ...   ...  ..  ..  ...     ...  ...    ...  ..   \n",
       "6630   0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "6631   0    0     0    0     1     0   0   0    0       0  ...      0   0   \n",
       "6632   0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "6633   0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "6634   0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "\n",
       "      zuo  zv  zvm  zw  zx  zy  zyxel  zz  \n",
       "0       0   0    0   0   0   0      0   0  \n",
       "1       0   0    0   0   0   0      0   0  \n",
       "2       0   0    0   0   0   0      0   0  \n",
       "3       0   0    0   0   0   0      0   0  \n",
       "4       0   0    0   0   0   0      0   0  \n",
       "...   ...  ..  ...  ..  ..  ..    ...  ..  \n",
       "6630    0   0    0   0   0   0      0   0  \n",
       "6631    0   0    0   0   0   0      0   0  \n",
       "6632    0   0    0   0   0   0      0   0  \n",
       "6633    0   0    0   0   0   0      0   0  \n",
       "6634    0   0    0   0   0   0      0   0  \n",
       "\n",
       "[6635 rows x 16384 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "X_train_countvec_df = pd.DataFrame(X_train.todense())\n",
    "\n",
    "# This are the different \"words\" that are in our vocabulary:\n",
    "X_train_countvec_df.columns = sorted(countvec.vocabulary_)\n",
    "print(X_train_countvec_df.columns)\n",
    "# This shows how a rows of our feature matrix look like:\n",
    "X_train_countvec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "610bc05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>001</th>\n",
       "      <th>0049</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>020</th>\n",
       "      <th>024246</th>\n",
       "      <th>...</th>\n",
       "      <th>zterm</th>\n",
       "      <th>zu</th>\n",
       "      <th>zuo</th>\n",
       "      <th>zv</th>\n",
       "      <th>zvm</th>\n",
       "      <th>zw</th>\n",
       "      <th>zx</th>\n",
       "      <th>zy</th>\n",
       "      <th>zyxel</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6630</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6631</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6632</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6634</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6635 rows × 16384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  0001  001  0049  00pm  01  02  020  024246  ...  zterm  zu  \\\n",
       "0      0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "1      0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "2      0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "3      0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "4      0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "...   ..  ...   ...  ...   ...   ...  ..  ..  ...     ...  ...    ...  ..   \n",
       "6630   0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "6631   0    0     0    0     1     0   0   0    0       0  ...      0   0   \n",
       "6632   0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "6633   0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "6634   0    0     0    0     0     0   0   0    0       0  ...      0   0   \n",
       "\n",
       "      zuo  zv  zvm  zw  zx  zy  zyxel  zz  \n",
       "0       0   0    0   0   0   0      0   0  \n",
       "1       0   0    0   0   0   0      0   0  \n",
       "2       0   0    0   0   0   0      0   0  \n",
       "3       0   0    0   0   0   0      0   0  \n",
       "4       0   0    0   0   0   0      0   0  \n",
       "...   ...  ..  ...  ..  ..  ..    ...  ..  \n",
       "6630    0   0    0   0   0   0      0   0  \n",
       "6631    0   0    0   0   0   0      0   0  \n",
       "6632    0   0    0   0   0   0      0   0  \n",
       "6633    0   0    0   0   0   0      0   0  \n",
       "6634    0   0    0   0   0   0      0   0  \n",
       "\n",
       "[6635 rows x 16384 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_countvec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4db2b1",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer\n",
    "\n",
    "The embeddings obtained by the [term frequency-inverse document frequency (TF-IDF)](https://en.wikipedia.org/wiki/Tf–idf) have often certain advantages compared to the ones obtained by the count vectorizer.\n",
    "\n",
    "**Extract `X_train_tfidf` and `X_test_tfidf` form the training and test data similar to for the count vectorizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f28045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a TF-IDF vectorizer\n",
      "done in 1.264236s at 7.831MB/s\n",
      "n_features: 16384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Add your code here\n",
    "X_train_tfidf =\n",
    "X_test_tfidf = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce240f4",
   "metadata": {},
   "source": [
    "## Applying the learning algorithms\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Based on the both sets of extracted features, we now apply logistic regression to build a generalized linear model. Please note the [algorithmic options of logistic regression of scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression): The choice of the 'solver' becomes relevant here as the dataset is not that small. We also note that the default choice in the method is **with $\\ell_2$-regularization** (with regularization parameter $C=1$). See also [these instructions](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) for some comments on the differnet options.\n",
    "\n",
    "We first use the count vectorizer encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8511ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of training of LogisticRegression(max_iter=150, multi_class='multinomial') with count vectorizer encoding:  2.621 s\n",
      "Mean accuracy of model LogisticRegression(max_iter=150, multi_class='multinomial') on training data with count vectorizer encoding:  0.9965335342878674\n",
      "Mean accuracy of model LogisticRegression(max_iter=150, multi_class='multinomial') on test data with count vectorizer encoding:  0.7475662214172515\n",
      "Runtime of evaluating LogisticRegression(max_iter=150, multi_class='multinomial') on training and test data with count vectorizer encoding:  0.010 s\n"
     ]
    }
   ],
   "source": [
    "y_train = train[\"target\"] \n",
    "y_test=test['target']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "t0 = time()\n",
    "lr =LogisticRegression(solver='lbfgs',max_iter=150,multi_class='multinomial').fit(X_train,train.target)\n",
    "print(\"Runtime of training of \"+str(lr)+\" with count vectorizer encoding: \",format(time()-t0,\"0.3f\"),\"s\")\n",
    "t0 = time()\n",
    "print(\"Mean accuracy of model \"+str(lr)+\" on training data with count vectorizer encoding: \",lr.score(X_train,y_train))\n",
    "print(\"Mean accuracy of model \"+str(lr)+\" on test data with count vectorizer encoding: \",lr.score(X_test,y_test))\n",
    "print(\"Runtime of evaluating \"+str(lr)+\" on training and test data with count vectorizer encoding: \",format(time()-t0,\"0.3f\"),\"s\")\n",
    "\n",
    "\n",
    "# We observe that the training time is reasonable, but not trivial anymore. On the other hand, evaluating the model (calculating the accuaracy) is still very quick\n",
    "# \n",
    "# Now, we repeat this with the TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0359b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of training of LogisticRegression(max_iter=150, multi_class='multinomial') with TF-IDF encoding:  1.565 s\n",
      "Mean accuracy of model LogisticRegression(max_iter=150, multi_class='multinomial') on training data with TF-IDF encoding:  0.9703089675960814\n",
      "Mean accuracy of model LogisticRegression(max_iter=150, multi_class='multinomial') on test data with TF-IDF encoding:  0.8021281412723568\n",
      "Runtime of evaluating LogisticRegression(max_iter=150, multi_class='multinomial') on training and test data with TF-IDF encoding:  0.008 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "lr2 =LogisticRegression(solver='lbfgs',max_iter=150,multi_class='multinomial').fit(X_train_tfidf,train.target)\n",
    "print(\"Runtime of training of \"+str(lr2)+\" with TF-IDF encoding: \",format(time()-t0,\"0.3f\"),\"s\")\n",
    "t0 = time()\n",
    "print(\"Mean accuracy of model \"+str(lr2)+\" on training data with TF-IDF encoding: \",lr2.score(X_train_tfidf,y_train))\n",
    "print(\"Mean accuracy of model \"+str(lr2)+\" on test data with TF-IDF encoding: \",lr2.score(X_test_tfidf,y_test))\n",
    "print(\"Runtime of evaluating \"+str(lr2)+\" on training and test data with TF-IDF encoding: \",format(time()-t0,\"0.3f\"),\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb6f3e",
   "metadata": {},
   "source": [
    "We note that the training of the logistic regression model takes considerably longer than the evaluation. The test accuracy for the TF-IDF encoding is better than for the counting encoding. \n",
    "The very high training accuracy suggests that we are in a situation where overfitting occurs.\n",
    "\n",
    "We can test the predictive quality of these models also on custom text documents. First, for the count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0eb4efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicuts of model LogisticRegression(max_iter=150, multi_class='multinomial') (count vectorizer encoding)\n",
      "'Bill Gates and Steve Jobs are computer enterpreneurs.' => comp.graphics\n",
      "'Elon Musk wants to fly to Mars.' => sci.space\n",
      "Predicuts of model LogisticRegression(max_iter=150, multi_class='multinomial') (TF-IDF encoding)\n",
      "'Bill Gates and Steve Jobs are computer enterpreneurs.' => comp.graphics\n",
      "'Elon Musk wants to fly to Mars.' => sci.space\n"
     ]
    }
   ],
   "source": [
    "text_test = ['Bill Gates and Steve Jobs are computer enterpreneurs.','Elon Musk wants to fly to Mars.']\n",
    "X_custom_counts=countvec.transform(text_test)\n",
    "X_custom_tfidf=vectorizer_tfidf.transform(text_test)\n",
    "\n",
    "predicted=lr.predict(X_custom_counts)\n",
    "predicted2=lr2.predict(X_custom_tfidf)\n",
    "\n",
    "print(\"Predicuts of model \"+str(lr)+\" (count vectorizer encoding)\")\n",
    "for doc,category in zip(text_test,predicted):\n",
    "    print('%r => %s'%(doc,train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b57eab-03ab-4e33-9bc4-c5d2b6a6d2ee",
   "metadata": {},
   "source": [
    "**Predict the categories from `text_test` using the TF-IDF embeddings, and print them analogously to above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02b087-364c-46f4-9049-213b55b670a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51238566",
   "metadata": {},
   "source": [
    "As we have been under the impression that overfitting has been occuring, we now run a cross-validation on $\\ell_2$-regularized logistic regression. We use \"LogisticRegressionCV\" instead of the generic method \"GridSearchCV\" (applied to a LogisticRegression) as this uses some tricks to make it computationally more efficient.\n",
    " \n",
    "However, running this cross validations for different regualarization parameters on a 5-fold split will still take a considerable amount of time. In practice, this could be run efficiently\n",
    "using distributed computing, even for larger data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d6dc7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of crossvalidation: 143.082 s\n",
      "Mean accuracy of model LogisticRegressionCV(Cs=20, cv=5, max_iter=150, multi_class='multinomial',\n",
      "                     random_state=10) on training data with TF-IDF encoding:  0.9966842501883949\n",
      "Mean accuracy of model LogisticRegressionCV(Cs=20, cv=5, max_iter=150, multi_class='multinomial',\n",
      "                     random_state=10) on test data with TF-IDF encoding:  0.8016753452569617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "# We focus on the TF-IDF model as it exhibited better performance above.\n",
    "t0 = time()\n",
    "lr_optimal = LogisticRegressionCV(Cs=20,cv=5, random_state=10,penalty='l2', max_iter=150,multi_class='multinomial',solver='lbfgs',refit=True).fit(X_train_tfidf, y_train)\n",
    "print(\"Runtime of crossvalidation:\",format(time()-t0,\"0.3f\"),\"s\")\n",
    "print(\"Mean accuracy of model \"+str(lr_optimal)+\" on training data with TF-IDF encoding: \",lr_optimal.score(X_train_tfidf,y_train))\n",
    "print(\"Mean accuracy of model \"+str(lr_optimal)+\" on test data with TF-IDF encoding: \",lr_optimal.score(X_test_tfidf,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f8b3f",
   "metadata": {},
   "source": [
    "We note that the cross validation was not successful in improving the accuracy on the hold-out test set.\n",
    " \n",
    "We plot the validation errors for the different regularization parameters $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64a1b2dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/PIL/_imaging.cpython-39-darwin.so, 0x0002): Library not loaded: @rpath/libtiff.5.dylib\n  Referenced from: <3FB9ACDB-C376-3306-A828-D7A27124C0E6> /Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/PIL/_imaging.cpython-39-darwin.so\n  Reason: tried: '/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/PIL/../../../libtiff.5.dylib' (no such file), '/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/PIL/../../../libtiff.5.dylib' (no such file), '/Users/ckuemme1/opt/anaconda3/bin/../lib/libtiff.5.dylib' (no such file), '/Users/ckuemme1/opt/anaconda3/bin/../lib/libtiff.5.dylib' (no such file), '/usr/local/lib/libtiff.5.dylib' (no such file), '/usr/lib/libtiff.5.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(lr_optimal\u001b[38;5;241m.\u001b[39mCs_,lr_optimal\u001b[38;5;241m.\u001b[39mscores_[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py:161\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/rcsetup.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/colors.py:52\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Real\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPngImagePlugin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PngInfo\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py:82\u001b[0m\n\u001b[1;32m     73\u001b[0m MAX_IMAGE_PIXELS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _imaging \u001b[38;5;28;01mas\u001b[39;00m core\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __version__ \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(core, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     85\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCore version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(core,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPillow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/PIL/_imaging.cpython-39-darwin.so, 0x0002): Library not loaded: @rpath/libtiff.5.dylib\n  Referenced from: <3FB9ACDB-C376-3306-A828-D7A27124C0E6> /Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/PIL/_imaging.cpython-39-darwin.so\n  Reason: tried: '/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/PIL/../../../libtiff.5.dylib' (no such file), '/Users/ckuemme1/opt/anaconda3/lib/python3.9/site-packages/PIL/../../../libtiff.5.dylib' (no such file), '/Users/ckuemme1/opt/anaconda3/bin/../lib/libtiff.5.dylib' (no such file), '/Users/ckuemme1/opt/anaconda3/bin/../lib/libtiff.5.dylib' (no such file), '/usr/local/lib/libtiff.5.dylib' (no such file), '/usr/lib/libtiff.5.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(lr_optimal.Cs_,lr_optimal.scores_[1][1])\n",
    "ax = plt.gca()\n",
    "ax.set_xscale('log')\n",
    "ax.set(xlabel='Parameter C', ylabel='Validation accuracy')\n",
    "ax.set_box_aspect(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883dbea",
   "metadata": {},
   "source": [
    "We note that the maximal validation accuracy is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(lr_optimal.scores_[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a0d41",
   "metadata": {},
   "source": [
    "**Can you explain why the maximal validation accuracy is considerably larger than the test accuracy?**\n",
    "\n",
    "To get a better idea where the misclassifications take place (i.e., in which categories), we plot the [confusion matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "y_test_predicted = lr_optimal.predict(X_test_tfidf)\n",
    "plt.figure(figsize=(30,18))\n",
    "ax1 = plt.gca()\n",
    "plot_confusion_matrix(lr_optimal,X_test_tfidf,y_test,display_labels=categories,ax=ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c125e",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors & Support Vector Machines (optional, do if you have time)\n",
    " \n",
    "Instead of logistic regression, we can also other classifiers.\n",
    "\n",
    "**Please use a k-nearest neighbor classifier, for different k, and a support vector machine classifier.**\n",
    "Print the training runtime for each model, the mean accuracy on training data and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_parameter =  # try different values of k\n",
    "t0 = time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf50ac",
   "metadata": {},
   "source": [
    "We observe that the performance of the nearest neighbors classifier is not very good in this setting (considering $5$ neighbors). Do you have any intuition why this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0cc8a",
   "metadata": {},
   "source": [
    "Now, perform cross-validation over the parameter k of k-nearest neighbors for the problem, using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcfdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "ks=np.arange(1,41,2) # create vector of logarithmically interpolated values between 10^(-5) and 10^(9)\n",
    "parameters = {'n_neighbors':ks}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75682822",
   "metadata": {},
   "source": [
    "**Find and provide the training and validation accuracies of the cross validation/gridsearch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37759ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = \n",
    "validation_accuracies = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02fde9c",
   "metadata": {},
   "source": [
    "Plotting the training and validation accuracies, we observe the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b86221",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ks,train_accuracies)\n",
    "plt.plot(ks,validation_accuracies)\n",
    "ax = plt.gca()\n",
    "ax.set(xlabel='alpha', ylabel='accuracy',\n",
    "       title='Cross-validation accuracies for k-Nearest Neighbors')\n",
    "ax.legend([\"training data\",\"validation data\"], loc=0)\n",
    "print(\"Best parameter k:\",str(gridsearch.best_params_['n_neighbors']))\n",
    "#ax.set_xticks()\n",
    "print(\"Mean accuracy of model \"+str(gridsearch.best_estimator_)+\" on test data with TF-IDF encoding: \",gridsearch.best_estimator_.score(X_test_tfidf,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e712f04",
   "metadata": {},
   "source": [
    "**How crossvalidated k-nn compare to the support vector machine you used?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0313d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "118c671a",
   "metadata": {},
   "source": [
    "**Is there a parameter you can \"cross-validate\" (optimize by cross validation) if using a support vector machine?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0090d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
